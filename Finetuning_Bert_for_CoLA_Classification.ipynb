{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/federaspa/Finetuning-Bert-for-Text-Classification/blob/main/Finetuning_Bert_for_CoLA_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7aeed1a"
      },
      "source": [
        "## Necessary imports and setup"
      ],
      "id": "c7aeed1a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bfcb505"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet \"datasets\" \"scikit-learn\" \"torchmetrics>=0.7\" \"scipy\" \"torch>=1.8\" \"transformers\" \"torchtext>=0.9\" \"setuptools==59.5.0\" \"ipython[notebook]\"\n",
        "%load_ext tensorboard"
      ],
      "id": "1bfcb505"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f23d15c9"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from typing import Optional\n",
        "\n",
        "import datasets\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")"
      ],
      "id": "f23d15c9"
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset from the HuggingFace dataset library\n",
        "dataset = datasets.load_dataset(\"glue\", \"cola\")\n",
        "# setup tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# setup the pretrained model\n",
        "config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', \n",
        "                                                           config=config)"
      ],
      "metadata": {
        "id": "lb2TT1nsgavX"
      },
      "id": "lb2TT1nsgavX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n"
      ],
      "metadata": {
        "id": "tJetkGYdmN__"
      },
      "id": "tJetkGYdmN__"
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_features(example_batch,  max_seq_length = max_seq_length, indices=None):\n",
        "\n",
        "    # tokenize the sentence\n",
        "    features = tokenizer.batch_encode_plus(\n",
        "        example_batch[\"sentence\"], \n",
        "        max_length=max_seq_length, \n",
        "        pad_to_max_length=True, \n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # rename label to labels to make it easier to pass to model forward\n",
        "    features[\"labels\"] = example_batch[\"label\"]\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "QamES0SrjejJ"
      },
      "id": "QamES0SrjejJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 32\n",
        "eval_batch_size = 64\n",
        "max_seq_length = 128\n",
        "\n",
        "# iterate over all splits in the dataset (train, validation and test)\n",
        "for split in dataset.keys():\n",
        "    \n",
        "    # replace each split with the tokenized version of itself, removing \"labels\"\n",
        "    dataset[split] = dataset[split].map(\n",
        "        convert_to_features,\n",
        "        batched=True,\n",
        "        remove_columns=[\"label\"],\n",
        "    )\n",
        "    columns = [c for c in dataset[split].column_names if c in [\"input_ids\", \"attention_mask\", \"labels\"]]\n",
        "\n",
        "    dataset[split].set_format(type=\"torch\", columns=columns)"
      ],
      "metadata": {
        "id": "XZ2yxH6shWlG"
      },
      "id": "XZ2yxH6shWlG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(dataset['train'], batch_size = train_batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=2)\n",
        "\n",
        "val_dl = DataLoader(dataset['validation'], batch_size=train_batch_size, shuffle=False, drop_last=False, num_workers=2)\n",
        "\n",
        "test_dl = DataLoader(dataset['test'], batch_size=eval_batch_size, shuffle=False, drop_last=False, num_workers=2)"
      ],
      "metadata": {
        "id": "cIU767uxkqqh"
      },
      "id": "cIU767uxkqqh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us indicise the first element of train to check if the tokenization was successful. We must call *list* due to the lazy nature of train_dataloader's computation."
      ],
      "metadata": {
        "id": "0p-e7D_knxnR"
      },
      "id": "0p-e7D_knxnR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czg86GX49Hw1"
      },
      "outputs": [],
      "source": [
        "list(train_dl)[0]"
      ],
      "id": "Czg86GX49Hw1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check our dataloaders' sizes."
      ],
      "metadata": {
        "id": "NhAJYUtvogVG"
      },
      "id": "NhAJYUtvogVG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDuoAKwvE-P1"
      },
      "outputs": [],
      "source": [
        "print(\"# Training iterations:\", len(train_dl))\n",
        "print(\"# Training sentences:\", len(train_dl.dataset))\n",
        "print(\"# Validation iterations:\", len(val_dl))\n",
        "print(\"# Validation sentences:\", len(val_dl.dataset))\n",
        "print(\"# Test iterations:\", len(test_dl))\n",
        "print(\"# Test sentences:\", len(test_dl.dataset))"
      ],
      "id": "vDuoAKwvE-P1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ZUnCEf5kHCBZ"
      },
      "id": "ZUnCEf5kHCBZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOnpTrRiz3PW"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def epoch(model, data_loader, logger,metric, mode=\"train\", optim=None, schedule = False,\n",
        "          device=\"cpu\", epoch_idx=0, dropout = None):\n",
        "    assert optim is not None or mode != \"train\", \\\n",
        "        \"Optimizer required in 'train' mode\"\n",
        "\n",
        "    # TODO: fill it yourself!\n",
        "\n",
        "    outputs_list = []\n",
        "    for batch_idx, batch in (pbar:= tqdm(enumerate(data_loader), total=len(data_loader))):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        labels = batch['labels']\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, axis=1)\n",
        "\n",
        "        if mode == \"train\":\n",
        "            loss.backward() \n",
        "            optim.step()\n",
        "            if schedule:\n",
        "                lr_scheduler.step()\n",
        "            optim.zero_grad()\n",
        "            logger.add_scalar(f\"{mode}/batch_loss\", loss, batch_idx + \n",
        "                              len(data_loader)*epoch_idx)\n",
        "\n",
        "        outputs_list.append({\"loss\": loss, \"preds\": preds, \"labels\": labels})\n",
        "        pbar.set_description(desc=f\"Epoch: {epoch_idx} - {mode}, Loss:{loss:.2f}\")\n",
        "\n",
        "    preds = torch.cat([x[\"preds\"] for x in outputs_list]).detach().cpu()\n",
        "    labels = torch.cat([x[\"labels\"] for x in outputs_list]).detach().cpu()\n",
        "    loss = torch.stack([x[\"loss\"] for x in outputs_list]).mean()\n",
        "    acc = metric(preds, labels)\n",
        "    \n",
        "    logger.add_scalar(f\"{mode}/loss\", loss, epoch_idx)\n",
        "    logger.add_scalar(f\"{mode}/acc\", acc, epoch_idx)\n",
        "\n",
        "    print(f\"Epoch: {epoch_idx} - {mode}, Loss: {loss:.2f}, Acc: {acc}\")\n",
        "    return"
      ],
      "id": "IOnpTrRiz3PW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KXcMt8lKItt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "metric = matthews_corrcoef"
      ],
      "id": "-KXcMt8lKItt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a5fCWX0A8Nk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_rows = 4000\n",
        "%tensorboard --logdir logs\n",
        "from tensorboard import notebook\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "tb_logger = SummaryWriter(f\"logs/{datetime.now()}\")\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 5\n",
        "learning_rate = 3e-5\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running training on device: {device}\")\n",
        "\n",
        "# Reinstantiate model to be sure we train a new model\n",
        "config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=num_labels, hidden_dropout_prob = 0.2, attention_probs_dropout_prob = 0.2)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', \n",
        "                                                           config=config)\n",
        "# Prepare optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# Prepare scheduler\n",
        "train_steps = epochs * len(train_dl)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=train_steps\n",
        ")\n",
        "# Perform training over the epochs\n",
        "model.to(device)\n",
        "\n",
        "for e in range(epochs):\n",
        "    # Training epoch\n",
        "    model.train()\n",
        "    epoch(model, train_dl, tb_logger, metric, \"train\", optimizer, schedule = True, device=device, epoch_idx=e)\n",
        "\n",
        "    # Evaluation epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        epoch(model, val_dl, tb_logger, metric, \"val\", device=device, schedule = True, epoch_idx=e)\n"
      ],
      "id": "2a5fCWX0A8Nk"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "jupytext": {
      "cell_metadata_filter": "colab,id,colab_type,-all",
      "formats": "ipynb,py:percent",
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 291.073524,
      "end_time": "2022-05-05T01:29:11.588039",
      "environment_variables": {},
      "exception": null,
      "input_path": "lightning_examples/text-transformers/text-transformers.ipynb",
      "output_path": ".notebooks/lightning_examples/text-transformers.ipynb",
      "parameters": {},
      "start_time": "2022-05-05T01:24:20.514515",
      "version": "2.3.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}